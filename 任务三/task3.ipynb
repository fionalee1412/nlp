{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "source": [
        "# 基本文本处理技能\n",
        "  1.1 分词的概念（分词的正向最大、逆向最大、双向最大匹配法）；\n",
        "  1.2 词、字符频率统计；（可以使用Python中的collections.Counter模块，也可以自己寻找其他好用的库）\n",
        "# 2. 语言模型\n",
        "\n",
        "  2.1 语言模型中unigram、bigram、trigram的概念；\n",
        "  2.2 unigram、bigram频率统计；（可以使用Python中的collections.Counter模块，也可以自己寻找其他好用的库）\n",
        "# 3. 文本矩阵化：要求采用词袋模型且是词级别的矩阵化\n",
        "步骤有：\n",
        "  3.1 分词（可采用结巴分词来进行分词操作，其他库也可以）；\n",
        "  3.2 去停用词；构造词表。\n",
        "  3.3 每篇文档的向量化。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## 1. 基本文本处理技能\n",
        "  ### 1.1 分词的概念（分词的正向最大、逆向最大、双向最大匹配法）；\n",
        "  ### 1.2 词、字符频率统计；（可以使用Python中的collections.Counter模块，也可以自己寻找其他好用的库）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({\u0027自\u0027: 2, \u0027然\u0027: 2, \u0027语\u0027: 2, \u0027言\u0027: 2, \u0027处\u0027: 2, \u0027理\u0027: 2, \u0027我\u0027: 1, \u0027爱\u0027: 1, \u0027。\u0027: 1, \u0027是\u0027: 1, \u0027一\u0027: 1, \u0027个\u0027: 1, \u0027很\u0027: 1, \u0027有\u0027: 1, \u0027意\u0027: 1, \u0027思\u0027: 1, \u0027的\u0027: 1, \u0027研\u0027: 1, \u0027究\u0027: 1, \u0027领\u0027: 1, \u0027域\u0027: 1})\n"
          ]
        }
      ],
      "source": [
        "text \u003d \u0027我爱自然语言处理。自然语言处理是一个很有意思的研究领域\u0027\n",
        "from collections import Counter\n",
        "c \u003d Counter(text)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "词频统计相比字符统计而言，只是多了一步分词的过程，具体代码如下所示："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dumping model to file cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading model cost 1.375 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Prefix dict has been built succesfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({\u0027自然语言\u0027: 2, \u0027处理\u0027: 2, \u0027。\u0027: 2, \u0027我\u0027: 1, \u0027爱\u0027: 1, \u0027是\u0027: 1, \u0027一个\u0027: 1, \u0027很\u0027: 1, \u0027有意思\u0027: 1, \u0027的\u0027: 1, \u0027研究\u0027: 1, \u0027领域\u0027: 1})\n"
          ]
        }
      ],
      "source": [
        "import jieba \n",
        "seg_list \u003d list(jieba.cut(\u0027我爱自然语言处理。自然语言处理是一个很有意思的研究领域。\u0027, cut_all\u003dFalse)) \n",
        "c \u003d Counter(seg_list )\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# 2. 语言模型\n",
        "\n",
        "  2.1 语言模型中unigram、bigram、trigram的概念；\n",
        "  2.2 unigram、bigram频率统计；（可以使用Python中的collections.Counter模块，也可以自己寻找其他好用的库）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "# 分词"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
            "Loading model cost 1.480 seconds.\n",
            "Prefix dict has been built succesfully.\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": [
            "[全模式]:  我/ 爱/ 深度/ 学习/ 的/ 自然/ 自然语言/ 语言/ 处理\n[精确模式]:  我/ 爱/ 深度/ 学习/ 的/ 自然语言/ 处理\n[默认模式]:  我/ 爱/ 深度/ 学习/ 的/ 自然语言/ 处理\n[搜索引擎模式]:  我/ 爱/ 深度/ 学习/ 的/ 自然/ 语言/ 自然语言/ 处理\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import jieba\n",
        "\n",
        "# 全模式\n",
        "text \u003d \"我爱深度学习的自然语言处理\"\n",
        "seg_list \u003d jieba.cut(text, cut_all\u003dTrue)\n",
        "print(u\"[全模式]: \", \"/ \".join(seg_list))\n",
        "\n",
        "# 精确模式\n",
        "seg_list \u003d jieba.cut(text, cut_all\u003dFalse)\n",
        "print(u\"[精确模式]: \", \"/ \".join(seg_list))\n",
        "\n",
        "# 默认是精确模式\n",
        "seg_list \u003d jieba.cut(text)\n",
        "print(u\"[默认模式]: \", \"/ \".join(seg_list))\n",
        "\n",
        "# 搜索引擎模式\n",
        "seg_list \u003d jieba.cut_for_search(text)\n",
        "print(u\"[搜索引擎模式]: \", \"/ \".join(seg_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# 去停用词"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "with open(r\u0027D:\\workspace-py\\datawhale\\nlp\\任务三\\task3.ipynb\u0027, \u0027r\u0027, encoding \u003d \u0027utf-8\u0027) as f:\n",
        "    content \u003d f.read()\n",
        "    stop_words \u003d content.split(\u0027\\n\u0027)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# 3.3向量化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-5-c544e590e592\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\u0027\\n\u0027\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcount_vectorizer\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#上一步的停用词\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 6\u001b[1;33m     \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mvec\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mvocab_list\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#得到字典\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    834\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m         \"\"\"\n\u001b[1;32m--\u003e 836\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X \u003d self._count_vocab(raw_documents,\n\u001b[1;32m--\u003e 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 811\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m    812\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ],
          "ename": "ValueError",
          "evalue": "empty vocabulary; perhaps the documents only contain stop words",
          "output_type": "error"
        }
      ],
      "source": "from sklearn.feature_extraction.text import CountVectorizer\nwith open(r\u0027D:\\workspace\\py_projects\\cp_git\\nlp\\任务2 - 数据集下载探索模块.md\u0027, \u0027r\u0027, encoding \u003d \u0027utf-8\u0027) as f:\n    content \u003d f.read()\n    stop_words \u003d content.split(\u0027\\n\u0027)\n    count_vectorizer \u003d CountVectorizer(stop_words\u003dstop_words) #上一步的停用词\n    count_vectorizer.fit(seg_list)\n    vec \u003d count_vectorizer.transform(seg_list).toarray()\n    vocab_list \u003d count_vectorizer.get_feature_names() #得到字典\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}